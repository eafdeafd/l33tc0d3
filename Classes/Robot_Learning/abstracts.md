Abstract Writing Instructions:

    1-2 sentences describing the problem
    1-2 sentences explaining why the state-of-the-art is not enough for this, why it fails
    1-2 sentences explaining the clever idea of this paper
    1-2 sentences explaining how the idea is implemented
    1 sentence about the experimental evaluationAbstract Summary Week 2 - 2D Vision

ImageNet Classification with Deep Convolutional Neural Networks:

The ImageNet competition pitted models against each other on a large dataset (1.2 million images) for object classification/recognition. Object classification is the correct detection of a class of object inside an image that only contains one type of class of object. Current state-of-the-art methods were often small in capacity and thus could not robustly learn the features of classes for large datasets such as ImageNet. AlexNet has several improvements to beat state-of-the-art methods with the most obvious being a wider and deeper CNN-based network trained on two GPUs. The other innovations were in the implementation of AlexNet which is an 8 layer network (5 conv, 3 fully connected) using ReLUs (non-saturating) instead of Tanh for the activation functions, grouping using cross-GPU training memory, local response normalization for feature maps while training, overlapping max pooling instead of non-overlapping, learning rate scheduler, Dropout, guassian initialization, and data augmentation with horizontal flipping and PCA for brightness/color intensity. AlexNet beats out other methods by a 7-10% error reduction in top-1 and top-5 error rates (37.5% and 17%) in the ILSVRC-2010 competition with ensembling multiple AlexNets reducing the error rates to 15.3% for top-5.
The ImageNet competition pitted models against each other on a large dataset (1.2 million images) for object classification/recognition. Object classification is the correct detection of a class of object inside an image that only contains one type of class of object. Current state-of-the-art methods were often small in capacity and thus could not robustly learn the features of classes for large datasets such as ImageNet. AlexNet has several improvements to beat state-of-the-art methods with the most obvious being a wider and deeper CNN-based network trained on two GPUs. The other innovations were in the implementation of AlexNet which is an 8 layer network (5 conv, 3 fully connected) using ReLUs (non-saturating) instead of Tanh for the activation functions, grouping using cross-GPU training memory, local response normalization for feature maps while training, overlapping max pooling instead of non-overlapping, learning rate scheduler, Dropout, guassian initialization, and data augmentation with horizontal flipping and PCA for brightness/color intensity. AlexNet beats out other methods by a 7-10% error reduction in top-1 and top-5 error rates (37.5% and 17%) in the ILSVRC-2010 competition with ensembling multiple AlexNets reducing the error rates to 15.3% for top-5.

You Only Look Once: Unified, Real-Time Object Detection:

Object Detection, a task for localizing different classes of objects inside an image with bounding boxes, is a hard task and is usually slow. Most state-of-the-art models run at below real-time frequencies (Fast R-CNN runs at .5 FPS) mostly due to the complex vision processing pipelines that are disjoint from each other and requires passing data around and tuning separate hyperparameters. YOLO combines all of this processing into a single network that runs at blazing fast speeds (45 FPS for base YOLO, lightweight YOLO with 155 FPS). YOLO is able to do this by framing Object Detection as a Regression problem and predicting class probabilities with bounding boxes. This is accomplished by training the CNN-based (GoogLeNet) network on a loss that takes into account object x, y, bounding box h, w, and confidence in class prediction. Intuitively, the image is divided into a grid where the network predicts class probabilities for each grid cell and arbitrary amounts of bounding boxes for each grind cell along with confidence, and both are combined to produce a final prediction of larger bounding boxes and confidence. YOLO was evaluated on PASCAL VOC 2007 and 2012 datasets and although does not outperform the state-of-the-art models in mean average precision (falls short around 6-7%), it heavily outperforms them on FPS. Also, since the nature of the average errors of YOLO (localization errors) and Fast R-CNN models (background errors) are different, both can be ensembled into a detector that pushes the state-of-the-art accuracy.
Object Detection, a task for localizing different classes of objects inside an image with bounding boxes, is a hard task and is usually slow. Most state-of-the-art models run at below real-time frequencies (Fast R-CNN runs at .5 FPS) mostly due to the complex vision processing pipelines that are disjoint from each other and requires passing data around and tuning separate hyperparameters. YOLO combines all of this processing into a single network that runs at blazing fast speeds (45 FPS for base YOLO, lightweight YOLO with 155 FPS). YOLO is able to do this by framing Object Detection as a Regression problem and predicting class probabilities with bounding boxes. This is accomplished by training the CNN-based (GoogLeNet) network on a loss that takes into account object x, y, bounding box h, w, and confidence in class prediction. Intuitively, the image is divided into a grid where the network predicts class probabilities for each grid cell and arbitrary amounts of bounding boxes for each grind cell along with confidence, and both are combined to produce a final prediction of larger bounding boxes and confidence. YOLO was evaluated on PASCAL VOC 2007 and 2012 datasets and although does not outperform the state-of-the-art models in mean average precision (falls short around 6-7%), it heavily outperforms them on FPS. Also, since the nature of the average errors of YOLO (localization errors) and Fast R-CNN models (background errors) are different, both can be ensembled into a detector that pushes the state-of-the-art accuracy.

Mask R-CNN:

Instance Segmentation is the problem of segmenting multiple class instances from a single image (separate pixel mappings). Current state-of-the-art models struggle to bridge the gap between the success of instance detection methods (Faster R-CNN, YOLO) and semantic segmentation methods (FCN), usually relying on one of these methods and performing subpar. Mask R-CNN bridges this gap by adding to the core idea of Faster R-CNN with a separate branch to predict segmentation masks along with the bounding box and classification at the same time. Mask R-CNN also delivers the key idea of using RoI Align, which keeps pixel-to-pixel alignment for the semantic masks instead of RoI Pool or RoI Warp that distort these alignments for the sake of reducing feature space. Mask R-CNN was implemented using a backbone feature extractor (ResNet, ResNext, etc.) and a head (FCN, MLP) for bounding box prediction/classification and mask output, all trained with a loss function that includes classification, bounding box, and mask loss. Mask R-CNN outperforms all previous SOTA methods at its baseline model on the COCO data set (37.1% best AP vs. FCIS 33.6%) and causality was tested with ablation experiments (backbone, multinomial vs. independent masks, RoI Align, head).

Dex-Net 1.0: A cloud-based network of 3D objects for robust grasp planning using a Multi-Armed Bandit model with correlated rewards

Grasping a 3D object with a proper amount of grip force using a robot parallel-jaw gripper arm and uncertainty in the object position with only a camera is a very challenging problem. Previous attempts were either too low scale in nature or did not tackle the 3D model object case. The core idea of this paper is to massively increase the computation with a large dataset of 10,000 models and 2.5 million grasps and to run it on GCP to parallelize this massive effort and reduce runtime drastically. It also introduces a new concept of Multi-View CNNs for 3D object classification and a similarity metric to try and seed guesses from previously learned grasped items and their shape. This model was implemented by using Antipodal Grasp Sampling and the MV-CNN to seed priors from images (grasp sampling, grasp heightmap, object similarity) to produce a belief distribution model, then the Multi-Armed Bandit RL algorithm is run to evaluate the real force needed and update the belief. This was evaluated with their own dataset that was made by conglomerating several existing ones and tested against Tompson sampling without priors and uniform allocation as well as varying levels of prior size and kernel hyperparameters of DexNet.

Learning to Look Around: Intelligently Exploring Unseen Environments for Unknown Tasks

Active perception is an open problem with many subsets, one of those subset of problems is "observation completion" where an image and scene/object needs to be completed by an agent that learns optimal viewpoint exploration for completion. Generally, the class of problem falls under agents that "learn to observe" better. Current SOTA doesn't quite tackle the challenge in the same manner that the paper proposes, there are image completion methods that have passive agents and reconstruction without learning action policies and more of a focus on the completion part. None tackle learning an action policy for better image completion. The paper's key idea is to combine temporal-based completion with a learned action policy via REINFORCE to generate policies for efficient viewpoint exploration. It was implemented in five stages - Sense, which takes the proprioception stack and image and outputs two latent feature vectors (conv layers + FC), Fuse, which fuses these two vectors into one (two FC layers), Aggregate, which is an LSTM that keeps track of these feature vectors through time, Act, which samples an action from the policy using this feature vector (FC layers), and Decode, which upsamples the feature vector from Aggregate and outputs a guess of the entire viewgrid (up convs). This model was evaluated on SUN360 and ModelNet with baselines of one-viewing of the image, random policy, largest-action of policy, and peek-saliency viewing and outperforms all of them by 6-10%, getting increasingly better outperformance on unseen classes.

Embodied Amodal Recognition: Learning to Move to Perceive Objects
Amodal recognition is recognizing occluded objects given a 2D image. Embodied Amodal Recognition takes this one step forward - recognizing an occluded object in a 2D image with an agent that can be controlled in a 3D environment. Current SOTA methods do not tackle this task, they instead work on constrained versions where either there is no movement required or the agent does not move itself in a 3D space. This paper provides a first jab at the new task with embodied Mask R-CNN, which combines the normal Mask R-CNN and adds temporality with a Conv GRU and adds a policy with an action network trained with REINFORCE. The amodal recognition model is implemented by extracting a feature map using a conv network, temporal features with conv GRU, and final RoI head network. The policy network takes the current action via MLP, previous actions via GRU, and encoded image features via conv layers and outputs an action that is learned via REINFORCE with the reward function being the classification accuracy and IoU from the modal recognition model. The model is evaluated with a simulated 3D indoor environment against baselines such as passive viewing models and shortest path policies and mostly outperforms all of them; however, it should be noted that the gain from passive viewing models to embodiment (.2%-1% on Class-acc to IoU) is around the same as the gain from shortest path models to their policy (.5 - 1%).

PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space (2)
3D perception tasks such as object detection and segmentation are harder than 2D challenges because of the varied representation 3D objects could have (explicit vs. implicit) and the lack of development in this area in general. The current state of the art (PointNet) is not enough because it fails to represent the local structure of objects and struggles with point clouds of different densities. This can be remedied by building a bottom-up hierarchal representation of the scene similar to convolutional networks by interchanging layers of sampling, grouping, and feature extraction and by training on input dropout. Each layer can be abstracted to a "set abstraction" similar to a convolutional layer. The sampling stage uses iterative farthest point to sample centroids from a point cloud, then the grouping uses a ball query to select neighboring points, and finally, the features from these groups are extracted using PointNet as the feature extractor. The models (MSG, MRG, etc.) were evaluated on MNIST, ModelNet40, SHREC15, and ScanNet datasets outperforming vanilla PointNet, Multi-view CNN, and others, especially on sparse point clouds.

DenseFusion: 6D Object Pose Estimation by Iterative Dense Fusion
6D Object Pose estimation is the problem of finding a set of poses and rotations for many objects in an RBG-D image. Previous SOTA methods fail due to being slow or not processing RGB and depth data fused together. DenseFusion combines the depth and RGB data of an image per pixel and uses an iterative refinement method built into the architecture. DenseFusion is implemented using a two-stage approach: first segment out to produce masks for each object category using an encoder-decoder architecture, then combine the latent depth and color embedding per pixel (CNN, PointNet) and output a guess on the 6D pose (MLP, avg pooling, pose predictor). It was evaluated with the YCB-Video Dataset and LineMOD dataset against PointFusion and PoseCNN+ICP, outperforming both, especially on time (~3% better ADD-S<2cm and two magniutdes faster).

NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis
View synthesis is essentially scene completion - given a couple of images of an object or scene, generate any arbitrary new image from any viewing angle of the object or scene. Current SOTA fails to adequately represent the object or scene in a space-efficient manner and is inaccurate at the details of the generated image. NeRF proposes a novel solution by using a NN to represent the entire scene and requiring only the viewing direction and spatial location as the input and it will output a density and color at that spatial location. NeRF is an MLP that utilizes volume rendering by shooting a ray through the scene per pixel of the viewing camera angle and integrating over all points hit + the color that was hit to generate a pixel RGB output. Several tricks make NeRF better such as density independence on viewing angle, positional encoding for finer details, and hierarchal volume sampling for efficiency. A key insight is that the whole pipeline is differentiable, making NeRF highly efficient. NeRF was compared to baselines such as SRN, NV, and LLFF on real-world scenes and DeepVoxels dataset, and generally outperformed them all, especially in the finer details of an image and memory storage wise.
